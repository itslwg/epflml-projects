* EDA
 
  Import packages

#+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
import os
os.chdir("utils")
#+END_SRC

#+RESULTS:
:results:
0 - 7573792b-3d83-4698-99aa-4fdaa3c4b398
:end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime as dt
import itertools as it

from abc import ABC, abstractmethod

from implementations import reg_logistic_regression
from helpers import sigmoid, create_csv_submission
from cross_validation import accuracy, f1_score
from costs import reg_logistic_error
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[136]:
  :end:

  Setup notebook settings
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
%matplotlib inline
%load_ext autoreload
%autoreload 2
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[3]:
  :end:
  

  Read the training data

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
train = pd.read_csv("../data/train.csv")
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[137]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X = train.replace(to_replace=-999, value=np.nan)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[138]:
  :end:
  
  Replace the missing values by np.nan, and try list-wise deletion

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
cc = X.dropna()
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[139]:
  :end:

  We can also replace by the median of each corresponding column, and insert missing indicator variables
  for the missingness

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_imputed = X.fillna(value=X.median(), axis=0)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[140]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
nms = X.columns[X.isnull().sum() > 0]
mi = X[nms].where(X[nms].isnull(), 1).replace(to_replace=np.nan, value=0)
mi.columns = labels = list(np.add(nms, "__missing"))
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[141]:
  :end:
  
  Merge the missing indicators with the original dataset

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_imputed = pd.concat([
    X_imputed,
    mi
], axis=1)
X_imputed
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[142]:
  #+BEGIN_EXAMPLE
    Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  \
    0       100000          s       138.470                       51.655
    1       100001          b       160.937                       68.768
    2       100002          b       112.406                      162.172
    3       100003          b       143.905                       81.417
    4       100004          b       175.864                       16.915
    ...        ...        ...           ...                          ...
    249995  349995          b       112.406                       71.989
    249996  349996          b       112.406                       58.179
    249997  349997          s       105.457                       60.526
    249998  349998          b        94.951                       19.362
    249999  349999          b       112.406                       72.756

    DER_mass_vis  DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  \
    0             97.827    27.980                 0.910           124.711
    1            103.235    48.146                 2.107           225.885
    2            125.953    35.635                 2.107           225.885
    3             80.943     0.414                 2.107           225.885
    4            134.805    16.405                 2.107           225.885
    ...              ...       ...                   ...               ...
    249995        36.548     5.042                 2.107           225.885
    249996        68.083    22.439                 2.107           225.885
    249997        75.839    39.757                 2.107           225.885
    249998        68.812    13.504                 2.107           225.885
    249999        70.831     7.479                 2.107           225.885

    DER_prodeta_jet_jet  DER_deltar_tau_lep  ...  \
    0                     2.666               3.064  ...
    1                    -0.244               3.473  ...
    2                    -0.244               3.148  ...
    3                    -0.244               3.310  ...
    4                    -0.244               3.891  ...
    ...                     ...                 ...  ...
    249995               -0.244               1.392  ...
    249996               -0.244               2.585  ...
    249997               -0.244               2.390  ...
    249998               -0.244               3.365  ...
    249999               -0.244               2.025  ...

    DER_deltaeta_jet_jet__missing  DER_mass_jet_jet__missing  \
    0                                 1.0                        1.0
    1                                 0.0                        0.0
    2                                 0.0                        0.0
    3                                 0.0                        0.0
    4                                 0.0                        0.0
    ...                               ...                        ...
    249995                            0.0                        0.0
    249996                            0.0                        0.0
    249997                            0.0                        0.0
    249998                            0.0                        0.0
    249999                            0.0                        0.0

    DER_prodeta_jet_jet__missing  DER_lep_eta_centrality__missing  \
    0                                1.0                              1.0
    1                                0.0                              0.0
    2                                0.0                              0.0
    3                                0.0                              0.0
    4                                0.0                              0.0
    ...                              ...                              ...
    249995                           0.0                              0.0
    249996                           0.0                              0.0
    249997                           0.0                              0.0
    249998                           0.0                              0.0
    249999                           0.0                              0.0

    PRI_jet_leading_pt__missing  PRI_jet_leading_eta__missing  \
    0                               1.0                           1.0
    1                               1.0                           1.0
    2                               1.0                           1.0
    3                               0.0                           0.0
    4                               0.0                           0.0
    ...                             ...                           ...
    249995                          0.0                           0.0
    249996                          0.0                           0.0
    249997                          1.0                           1.0
    249998                          0.0                           0.0
    249999                          0.0                           0.0

    PRI_jet_leading_phi__missing  PRI_jet_subleading_pt__missing  \
    0                                1.0                             1.0
    1                                1.0                             0.0
    2                                1.0                             0.0
    3                                0.0                             0.0
    4                                0.0                             0.0
    ...                              ...                             ...
    249995                           0.0                             0.0
    249996                           0.0                             0.0
    249997                           1.0                             0.0
    249998                           0.0                             0.0
    249999                           0.0                             0.0

    PRI_jet_subleading_eta__missing  PRI_jet_subleading_phi__missing
    0                                   1.0                              1.0
    1                                   0.0                              0.0
    2                                   0.0                              0.0
    3                                   0.0                              0.0
    4                                   0.0                              0.0
    ...                                 ...                              ...
    249995                              0.0                              0.0
    249996                              0.0                              0.0
    249997                              0.0                              0.0
    249998                              0.0                              0.0
    249999                              0.0                              0.0

    [250000 rows x 43 columns]
  #+END_EXAMPLE
  :end:
  
  
  We are ultimately left with a sample of 68 thousand rows, which should be an adequate sample size for prediction.
  We also want to investigate the balance of the outcome. First, subset and coerce the label vector to numeric
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
y = np.where(np.asarray(cc.loc[:, 'Prediction']) == 's', 1, 0)
y
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[143]:
  : array([1, 0, 1, ..., 1, 1, 0])
  :end:

  Plot the outcome variable

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
unique, counts = np.unique(y, return_counts=True)
plt.bar(unique, counts)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[144]:
  : <BarContainer object of 2 artists>
  [[file:./obipy-resources/afOFtS.png]]
  :end:

  We have approximately 30 thousand labels that are classified as 1, and we see that the outcome is balanced among 1s 
  ans 0s. Hence, we are able to evaluate the model on a balanced outcome.
  
  Now we create the feature set. Drop the Prediction and the id columns to create the feature matrix
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
c = ['Id', 'Prediction']
X = cc.drop(columns=c)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[145]:
  :end:
  
  Standardize the feature set for prediction

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[146]:
  :end:

  Augment the dataset with 1s, for the intercept of the model.

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_model = pd.concat([
    pd.DataFrame(
        np.ones((X_standardized.shape[0], 1)), 
        columns=['beta0'], 
        index=X_standardized.index
    ),
    X_standardized
], axis=1)
X_model.iloc[:5, :5]
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[147]:
  #+BEGIN_EXAMPLE
    beta0  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h
    0     1.0      0.257669                     0.532831      0.474607 -1.093446
    5     1.0     -0.544384                    -0.706345     -0.475040  0.064383
    6     1.0      0.426949                    -0.208398      0.719029 -0.069451
    11    1.0     -0.132872                    -0.812491     -0.068375 -1.056286
    23    1.0      0.307232                    -1.123057      0.812305  0.820829
  #+END_EXAMPLE
  :end:
  
  To conduct the same data-preprocessing on the test set, we prepare a function for these operations
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def missing_data_handling(raw_sample, method="cc", imp_percentage=None):
    """
    Handle missing data for the raw sample

    Parameters
    ----------
    raw_sample: Numpy array
        Sample to be handled.
    method: String
        Missing data handler. Must be one of 'cc' (for complete case),
        'si' (for simple, median imputation), 'mediani' (for median, missing
        indicator imputation), or 'meani' (for mean, missing indicator imputation)
    imp_percentage: Float
        If method is 'si' and imp_percentage is specified, then only
        the features with proportions of missing data corresponding to
        imp_percentage will be imputed, the rest are removed. 
        If None (as default), all columns with missing data are handled
        using imputation.

    Returns
    -------
    raw_sample: Numpy array
        Sample with missing data handled.
    """
    assert method in ['cc', 'si', 'mediani', 'meani'], "Parameter method must be one of 'cc', 'si', or 'mi'"
    sample = raw_sample.replace(to_replace=-999, value=np.nan)
    
    if method == 'cc':
        sample = sample.dropna()
    elif method == 'si':
        sample = sample.fillna(value=sample.median(), axis=0)
    elif method == 'mediani' or method == 'meani':
        imputed = ""
        if method == 'mediani':
            imputed = sample.fillna(value=sample.median(), axis=0)
        else:
            imputed = sample.fillna(value=sample.mean(), axis=0)
        # Create missing indicator columns
        nms = sample.columns[sample.isnull().sum() > 0]
        mindicator = sample[nms].where(sample[nms].isnull(), 1).replace(to_replace=np.nan, value=0)
        mindicator.columns = list(np.add(nms, "__missing"))
        # Merge samples
        sample = pd.concat([
            imputed,
            mindicator
        ], axis=1)
        
    return sample 

def conduct_data_preparation(raw_sample, missing_method="cc", 
                             include_outcome=True):
    """Missing data handling and data subsetting."""
    ## Drop nas
    sample = missing_data_handling(
        raw_sample,
        method=missing_method
    )
    ## Subset outcome 
    y = None
    if include_outcome:
        y = np.where(np.asarray(sample.loc[:, 'Prediction']) == 's', 1, 0)
    c = ['Id', 'Prediction']
    sample = sample.drop(columns=c)

    return sample, y


def prepare_features(X):
    """Missing data handling and augmentation."""
    ## Standardize sample
    X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)
    ## Make prediction data
    X_model = pd.concat([
        pd.DataFrame(
            np.ones((X_standardized.shape[0], 1)), 
            columns=['beta0'], 
            index=X_standardized.index
        ),
        X_standardized
    ], axis=1)

    return X_model
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[148]:
  :end:

  Prepare the data with missing imputation

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X, y = conduct_data_preparation(train, missing_method='meani')
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[149]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X.head()
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[150]:
  #+BEGIN_EXAMPLE
    DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \
    0    138.470000                       51.655        97.827    27.980
    1    160.937000                       68.768       103.235    48.146
    2    121.858528                      162.172       125.953    35.635
    3    143.905000                       81.417        80.943     0.414
    4    175.864000                       16.915       134.805    16.405

    DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \
    0              0.910000         124.71100             2.666000
    1              2.403735         371.78336            -0.821688
    2              2.403735         371.78336            -0.821688
    3              2.403735         371.78336            -0.821688
    4              2.403735         371.78336            -0.821688

    DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  ...  \
    0               3.064      41.928     197.760  ...
    1               3.473       2.078     125.157  ...
    2               3.148       9.336     197.814  ...
    3               3.310       0.414      75.968  ...
    4               3.891      16.405      57.983  ...

    DER_deltaeta_jet_jet__missing  DER_mass_jet_jet__missing  \
    0                            1.0                        1.0
    1                            0.0                        0.0
    2                            0.0                        0.0
    3                            0.0                        0.0
    4                            0.0                        0.0

    DER_prodeta_jet_jet__missing  DER_lep_eta_centrality__missing  \
    0                           1.0                              1.0
    1                           0.0                              0.0
    2                           0.0                              0.0
    3                           0.0                              0.0
    4                           0.0                              0.0

    PRI_jet_leading_pt__missing  PRI_jet_leading_eta__missing  \
    0                          1.0                           1.0
    1                          1.0                           1.0
    2                          1.0                           1.0
    3                          0.0                           0.0
    4                          0.0                           0.0

    PRI_jet_leading_phi__missing  PRI_jet_subleading_pt__missing  \
    0                           1.0                             1.0
    1                           1.0                             0.0
    2                           1.0                             0.0
    3                           0.0                             0.0
    4                           0.0                             0.0

    PRI_jet_subleading_eta__missing  PRI_jet_subleading_phi__missing
    0                              1.0                              1.0
    1                              0.0                              0.0
    2                              0.0                              0.0
    3                              0.0                              0.0
    4                              0.0                              0.0

    [5 rows x 41 columns]
  #+END_EXAMPLE
  :end:
  

** Exploring the distributions of the variables

   #+BEGIN_SRC ipython :session :exports both :results drawer :async t
f = X.hist(figsize=(15, 15), bins = 100)
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[91]:
   [[file:./obipy-resources/FlHXxv.png]]
   :end:
   
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
corr = X.corr()
f, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(
    corr, 
    xticklabels=corr.columns.values,
    yticklabels=corr.columns.values,
    ax=ax
)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[99]:
   : <AxesSubplot:>
   [[file:./obipy-resources/MddO1k.png]]
   :END:
   
* Training
  :PROPERTIES:
  :ORDERED:  t
  :END:

  Define a function similar to that in implementations, but constructed to work with dataframes

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def split_data(x, y, ratio, shuffle=True, seed=1):
    """Split data into train and test set."""

    split = int(x.shape[0]*ratio)
    
    if shuffle:
        np.random.seed(seed)
        train_idx = np.random.permutation(np.arange(x.shape[0]))[:split]
        test_idx = np.random.permutation(np.arange(x.shape[0]))[split:]


        x_train = x.iloc[train_idx]
        y_train = y[train_idx]
        x_test = x.iloc[test_idx]
        y_test = y[test_idx]

    else:
        x_train = x.iloc[:split, :]
        y_train = y[:split]
        x_test = x.iloc[split:, :]
        y_test = y[split:]
    
    return x_train, x_test, y_train, y_test
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[7]:
  :end:
  
  Now, we don't want to conduct the data preparation on the full sample, as that would 
  cause information leakage, and biased oos performance. Hence we want to first split the training data

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_train, X_test, y_train, y_test =  split_data(X, y, 0.9)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[53]:
  :end:

  and then prepare the features
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_train = prepare_features(X_train)
X_test = prepare_features(X_test)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[54]:
  :end:

  Let's try to fit a regularized logistic regression model
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
w, loss = reg_logistic_regression(
    y=y_train.reshape(-1, 1),
    tx=np.asarray(X_train),
    lambda_=0.0001,
    reg=2,
    initial_w=np.array([-0.01 for x in X_train.columns])[np.newaxis].T,
    max_iters=100,
    gamma=0.9,
    batch_size=1,
    verbose=True
)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[105]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
preds = np.rint(sigmoid(X_test @ w))
accuracy(y_test.reshape(-1, 1), preds)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[106]:
  : 0.7536
  :end:
  
  
  Considering that all our models are similar in construction, and have similar method attached to them, let's define
  base classes for classifiers that enforce certain methods and a subclass for the specific classifier.
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
class Classifier(ABC):
    """Metaclass for classifier classes."""

    def __init__(self):
        pass

    @abstractmethod
    def fit(self, y, tx):
        raise NotImplementedError

    @abstractmethod
    def predict(self, w, tx):
        raise NotImplementedError

    @abstractmethod
    def compute_loss(self, y, tx, w):
        raise NotImplementedError
    

class RegularizedLogisticRegression(Classifier):
    """Implementation of regularized logistic regression."""

    def __init__(self, gamma, lambda_, reg):
        self.gamma = gamma
        self.lambda_ = lambda_
        self.reg = reg

    def fit(self, y, tx):
        return self.__reg_logistic_regression(y, tx)

    def predict(self, w, oos):
        return np.rint(sigmoid(oos @ w))

    def __reg_logistic_regression(self, y, tx):

        return reg_logistic_regression(
            y=y,
            tx=tx,
            lambda_=self.lambda_,
            reg=self.reg,
            initial_w=np.zeros((tx.shape[1], 1)),
            max_iters=100,
            gamma=self.gamma,
            batch_size=1  
        )

    def compute_loss(self, y, tx, w):
        
        return reg_logistic_error(
            y=y,
            tx=tx,
            w=w,
            lambda_=self.lambda_,
            reg=self.reg
        )
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[107]:
  :end:

  Define function for selecting the ks for cross-validation (from the lab)
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def build_k_indices(y, k_fold, seed):
    """Build k indices for k-fold."""
    num_row = y.shape[0]
    interval = int(num_row / k_fold)
    np.random.seed(seed)
    indices = np.random.permutation(num_row)
    k_indices = [indices[k * interval: (k + 1) * interval]
                 for k in range(k_fold)]
    return np.array(k_indices)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[108]:
  :end:
  
  Define the cross-validation function, utilizing the build_k_indices,
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def cross_validation(y, x, k_indices, comb,
                     classifier=RegularizedLogisticRegression):
    """Return the loss of ridge regression."""
        
    test_losses = [0] * k_indices.shape[0]
    test_acc = [0] * k_indices.shape[0]
    for k in np.arange(k_indices.shape[0]):
        # Augment and set indices
        mask = np.arange(k_indices.shape[0]) == k
        tri = k_indices[~mask].ravel()
        tei = k_indices[mask].ravel()
        # Subset for trainin and test sets
        x_train = x[tri]
        x_test = x[tei]
        y_train = y[tri]
        y_test = y[tei]
        # Run prediction
        clf = classifier(**comb)
        w, loss = clf.fit(y_train, x_train)
        test_losses[k] = clf.compute_loss(
            y=y_test,
            tx=x_test,
            w=w
        )
        predictions = clf.predict(w, x_test)
        test_acc[k] = accuracy(y_test, predictions)

    return np.mean(test_losses), np.mean(test_acc)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[109]:
  :end:

  Utilise a grid of hyperparameters, and a grid-search, for selecting the best hyperparameters for
  the model
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def select_best_model(grid, y, X, k_fold, seed=1, verbose=False):
    """Evaluate candidate model over grid of hyperparameters.

    Parameters
    ----------
    grid: Dict
         The hyperparameter grid, the keys being the hyperparamaters
         and corresponding lists are the potential values.

    Returns
    -------
    loss_te : Real scalar
         The loss of the best performing candiate model, measures
         by its oos accuracy.
    best_params : Dictionary
         Dictionary of parameter-value combinations 
         for the best performing candidate model.
    """
    s = np.random.seed(seed)
    k_indices = build_k_indices(y, k_fold, s)
    k, v = zip(*grid.items())
    permutations = [dict(zip(k, values)) for values in it.product(*v)]

    ## Reshape data to correct format
    y = y.reshape(-1, 1)
    test_loss = [0] * len(permutations)
    test_acc = [0] * len(permutations)
    for k, comb in enumerate(permutations):
        np.random.seed(seed)
        teloss, teacc = cross_validation(
            y=y,
            x=X,
            k_indices=k_indices,
            comb=comb
        )
        if verbose:
            print("For Params: " + str(comb) +
                  "\n \t Mean Accuracy: " + str(teacc),
                  ", Mean Log. loss: " + str(teloss))

        test_loss[k] = teloss
        test_acc[k] = teacc
            
    acc = max(test_acc)
    loss = min(test_loss)
    best_params = permutations[np.argmax(test_acc)]

    return (acc, loss, best_params)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[110]:
  :end:
  
  Test using simple grid. We do the hyperparameter tuning on training set, and the corresponding test sets
  in the cross validation are validation sets, rather than test sets. The test set is left for the
  last testing.
 
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
grid = {
    'gamma': np.arange(0.5, 1, 0.1),
    'lambda_': [0.001],
    'reg': [1, 2]
}
acc, loss, params = select_best_model(grid, y_train, np.asarray(X_train), 3, verbose=True)
"Test accuracy: {acc}, Test loss: {loss}, Best parameters: {params}".format(
    acc=acc,
    loss=loss,
    params=params
)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[111]:
  : "Test accuracy: 0.7502977777777778, Test loss: 1.250866372781578, Best parameters: {'gamma': 0.8999999999999999, 'lambda_': 0.001, 'reg': 2}"
  :end:

  Train the model on the full training set using the "best" parameters found from the hyperparameter tuning.
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
clf = RegularizedLogisticRegression(
    ,**params
)
X, y = conduct_data_preparation(train, missing_method='meani')
X_model = prepare_features(X)
w, loss = clf.fit(tx=np.array(X_model), y=y.reshape(-1, 1))
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[113]:
  :end:
    
  Check predictions on the sample

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
predictions = np.rint(sigmoid(X_model @ w))
len(w)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[130]:
  : 42
  :end:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
acc = accuracy(
    y_targ=y,
    y_pred=np.array(predictions).ravel()
)
f1 = f1_score(
    y_targ=y,
    y_pred=np.array(predictions).ravel()
)
"Accuracy: {acc}, F1-score: {f1}".format(
    acc=acc, f1=f1
)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[115]:
  : 'Accuracy: 0.7504919999999999, F1-score: 0.5871670141301829'
  :end:
  
* Testing

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
test = pd.read_csv("../data/test.csv")
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[151]:
  :end:
  
  Create prediction data with the test set
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_test, _ = conduct_data_preparation(test, missing_method='meani', include_outcome=False)
X_model_test = prepare_features(X_test)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[155]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_model_test.head()
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[157]:
  #+BEGIN_EXAMPLE
    beta0  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \
    0    1.0      0.000000                     0.856955     -1.413408 -0.865549
    1    1.0     -0.295946                     0.515112      0.168668 -0.123768
    2    1.0     -0.077989                     0.196862      0.376431 -0.848157
    3    1.0      0.267555                    -0.527057      0.399408 -0.769695
    4    1.0     -0.912540                     0.946887     -0.553227  0.502601

    DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \
    0              0.000000      5.287572e-16        -1.722525e-16
    1              0.000000      5.287572e-16        -1.722525e-16
    2              0.000000      5.287572e-16        -1.722525e-16
    3              0.000000      5.287572e-16        -1.722525e-16
    4             -1.125162      7.641942e-01         2.548787e-01

    DER_deltar_tau_lep  DER_pt_tot  ...  DER_deltaeta_jet_jet__missing  \
    0           -1.886220   -0.733285  ...                      -0.640885
    1           -0.418231   -0.749691  ...                      -0.640885
    2            0.488205   -0.682689  ...                      -0.640885
    3            0.560002   -0.454431  ...                      -0.640885
    4           -1.725959    2.675514  ...                       1.560340

    DER_mass_jet_jet__missing  DER_prodeta_jet_jet__missing  \
    0                  -0.640885                     -0.640885
    1                  -0.640885                     -0.640885
    2                  -0.640885                     -0.640885
    3                  -0.640885                     -0.640885
    4                   1.560340                      1.560340

    DER_lep_eta_centrality__missing  PRI_jet_leading_pt__missing  \
    0                        -0.640885                    -1.224013
    1                        -0.640885                     0.816983
    2                        -0.640885                    -1.224013
    3                        -0.640885                    -1.224013
    4                         1.560340                     0.816983

    PRI_jet_leading_eta__missing  PRI_jet_leading_phi__missing  \
    0                     -1.224013                     -1.224013
    1                      0.816983                      0.816983
    2                     -1.224013                     -1.224013
    3                     -1.224013                     -1.224013
    4                      0.816983                      0.816983

    PRI_jet_subleading_pt__missing  PRI_jet_subleading_eta__missing  \
    0                       -0.640885                        -0.640885
    1                       -0.640885                        -0.640885
    2                       -0.640885                        -0.640885
    3                       -0.640885                        -0.640885
    4                        1.560340                         1.560340

    PRI_jet_subleading_phi__missing
    0                        -0.640885
    1                        -0.640885
    2                        -0.640885
    3                        -0.640885
    4                         1.560340

    [5 rows x 42 columns]
  #+END_EXAMPLE
  :end:
  
    
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
predictions = np.rint(sigmoid(X_model_test @ w))
predictions[predictions == 0] = -1
len(predictions)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[160]:
  : 568238
  :end:

  Generate submission
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
ids = test.Id
create_csv_submission(ids, predictions, "../predictions/submission_16102020.csv")
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[159]:
  :end:
  
