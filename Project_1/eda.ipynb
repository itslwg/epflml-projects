{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["(pyvenv-activate \"~/courses/Machine Learning/\")"]},{"cell_type":"markdown","metadata":{},"source":["## EDA\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Import packages\n\n    import os\n    os.chdir(\"utils\")\n\n0 - bfa00fdd-902b-4828-8471-e3c89b341441\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import datetime as dt\n    import seaborn as sns\n    %matplotlib inline\n    import os\n    %load_ext autoreload\n    %autoreload 2\n    \n    from implementations import reg_logistic_regression\n    from helpers import sigmoid\n    from cross_validation import accuracy, f1_score\n\n1 - 9d1c6224-2e62-460e-a899-2d6fd053ceff\n\nRead the training data\n\n    train = pd.read_csv(\"../data/train.csv\")\n\n2 - 6438e20a-a329-4cf5-8509-109980b1271f\n\nReplace the missing values by np.nan, and try list-wise deletion\n\n    cc = train.replace(to_replace=-999, value=np.nan).dropna()\n    cc.shape\n\n3 - 1899f13a-0b81-4177-b3d2-8fcbf3473553\n\nWe are ultimately left with a sample of 68 thousand rows, which should be an adequate sample size for prediction.\nWe also want to investigate the balance of the outcome. First, subset and coerce the label vector to numeric\n\n    y = np.where(np.asarray(cc.loc[:, 'Prediction']) == 's', 1, 0)\n    y\n\n4 - 868ee9ff-1a52-4155-891e-cc7473aa7c83\n\nPlot the outcome variable\n\n    unique, counts = np.unique(y, return_counts=True)\n    plt.bar(unique, counts)\n\n5 - 171961ac-19ab-45ba-8d8e-768bece6f00b\n\nWe have approximately 30 thousand labels that are classified as 1, and we see that the outcome is balanced among 1s \nans 0s. Hence, we are able to evaluate the model on a balanced outcome.\n\nNow we create the feature set. Drop the Prediction and the id columns to create the feature matrix\n\n    c = ['Id', 'Prediction']\n    X = cc.drop(columns=c)\n\n6 - b893918c-0838-4d00-ba19-c4e8a09ecd2e\n\nStandardize the feature set for prediction\n\n    X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n\n7 - acc82dc3-2c44-4429-ac78-4ae082d8df5d\n\nAugment the dataset with 1s, for the intercept of the model.\n\n    X_model = pd.concat([\n        pd.DataFrame(\n            np.ones((X_standardized.shape[0], 1)), \n            columns=['beta0'], \n            index=X_standardized.index\n        ),\n        X_standardized\n    ], axis=1)\n    X_model.iloc[:5, :5]\n\n8 - f11e70bc-952d-4f08-9db9-c1d1f0f254e0\n\nTo conduct the same data-preprocessing on the test set, we prepare a function for these operations\n\n    def missing_data_handling(raw_sample, method=\"cc\", imp_percentage=None):\n        \"\"\"\n        Handle missing data for the raw sample\n    \n        Parameters\n        ----------\n        raw_sample: Numpy array\n            Sample to be handled.\n        method: String\n            Missing data handler. Must be one of 'cc' (for complete case)\n            or 'si' (for simple, median imputation).\n        imp_percentage: Float\n            If method is 'si' and imp_percentage is specified, then only\n            the features with proportions of missing data corresponding to\n            imp_percentage will be imputed, the rest are removed. \n            If None (as default), all columns with missing data are handled\n            using imputation.\n    \n        Returns\n        -------\n        raw_sample: Numpy array\n            Sample with missing data handled.\n        \"\"\"\n        assert method in ['cc', ''], \"Parameter method must be one of 'cc', 'si'\"\n        \n        if method == 'cc':\n            raw_sample = raw_sample.replace(to_replace=-999, value=np.nan).dropna()\n        elif method == 'si':\n            pass\n    \n        return raw_sample\n    \n    def conduct_data_preparation(raw_sample, missing_method=\"cc\", \n                                 include_outcome=True):\n        \"\"\"Missing data handling and data subsetting.\"\"\"\n        ## Drop nas\n        sample = missing_data_handling(\n            raw_sample,\n            method=missing_method\n        )\n        ## Subset outcome \n        y = None\n        if include_outcome:\n            y = np.where(np.asarray(cc.loc[:, 'Prediction']) == 's', 1, 0)\n        c = ['Id', 'Prediction']\n        X = cc.drop(columns=c)\n    \n        return X, y\n    \n    \n    def prepare_features(X, include_outcome=True):\n        \"\"\"Missing data handling, data subsettting, and augmentation.\"\"\"\n        ## Standardize sample\n        X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n        ## Make prediction data\n        X_model = pd.concat([\n            pd.DataFrame(\n                np.ones((X_standardized.shape[0], 1)), \n                columns=['beta0'], \n                index=X_standardized.index\n            ),\n            X_standardized\n        ], axis=1)\n    \n        return X_model\n\n9 - 882976c7-2592-418f-8c60-7318ac961094\n\n    X, y = conduct_data_preparation(train)\n\n10 - aa3e6491-46f0-4e41-892c-e0e70e3d9a41\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exploring the distributions of the variables\n\n"]},{"cell_type":"markdown","metadata":{},"source":["    f = X.hist(figsize=(15, 15), bins = 100)\n\n11 - 6eb3381b-958e-496e-8a23-6c74b4dd13cd\n\n    corr = X.corr()\n    f, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(\n        corr, \n        xticklabels=corr.columns.values,\n        yticklabels=corr.columns.values,\n        ax=ax\n    )\n\n12 - f0bad7c0-7cd1-4f17-9b8d-c31a5d9435e3\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Define a function similar to that in implementations, but constructed to work with dataframes\n\n    def split_data(x, y, ratio, shuffle=True, seed=1):\n        \"\"\"Split data into train and test set.\"\"\"\n    \n        split = int(x.shape[0]*ratio)\n        \n        if shuffle:\n            np.random.seed(seed)\n            train_idx = np.random.permutation(np.arange(x.shape[0]))[:split]\n            test_idx = np.random.permutation(np.arange(x.shape[0]))[split:]\n    \n    \n            x_train = x.iloc[train_idx]\n            y_train = y[train_idx]\n            x_test = x.iloc[test_idx]\n            y_test = y[test_idx]\n    \n        else:\n            x_train = x.iloc[:split, :]\n            y_train = y[:split]\n            x_test = x.iloc[split:, :]\n            y_test = y[split:]\n        \n        return x_train, x_test, y_train, y_test\n\n13 - 42ebf20f-fc31-4bf7-98c7-6ef6065ce1eb\n\nNow, we don't want to conduct the data preparation on the full sample, as that would \ncause information leakage, and biased oos performance. Hence we want to first split the training data\n\n    X_train, X_test, y_train, y_test =  split_data(X, y, 0.9)\n    X_train = prepare_features(X_train)\n    X_test = prepare_features(X_test)\n    X_train.iloc[:5, :5]\n\n14 - 0982a7ae-f023-48a7-84d2-08342072c53a\n\n    X_train.iloc[:5, :5]\n\n15 - a13590ab-e3e6-4c6f-9490-0ed300c716e3\n\n    w, loss = reg_logistic_regression(\n        y=y_train.reshape(-1, 1),\n        tx=np.asarray(X_train),\n        lambda_=0.2,\n        reg=1,\n        initial_w=np.array([0 for x in X_train.columns])[np.newaxis].T,\n        max_iters=100,\n        gamma=0.00011,\n        batch_size=50\n    )\n    loss\n\n16 - 894ea345-0ec1-48bf-8e4e-b453f45b18c9\n\nEvaluate the oos performance\n\n    predictions = np.rint(sigmoid(X_test @ w))\n    predictions.head()\n\n17 - 07d49219-409e-414a-925e-7a3edfd50adb\n\n    acc = accuracy(\n        y_targ=y_test,\n        y_pred=np.array(predictions).ravel()\n    )\n    f1 = f1_score(\n        y_targ=y_test,\n        y_pred=np.array(predictions).ravel()\n    )\n    print(\"Accuracy: {acc}, F1-score: {f1}\".format(acc=acc, f1=f1))\n\n18 - 80ee957b-8c5c-4001-910f-19f537ad098a\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Testing\n\n"]},{"cell_type":"markdown","metadata":{},"source":["    test = pd.read_csv(\"../data/test.csv\")\n\n19 - 113dd582-331a-4817-9b8d-679698d8aa4b\n\nCreate prediction data with the test set\n\n    X_test, _ = create_prediction_data(test, False)\n    X_test.shape\n\n20 - cc39f6b6-8872-492f-ace1-bd285da307e5\n\n    predictions = np.rint(sigmoid(X_test @ w))\n\n21 - b27fcf80-e99e-4ab2-9fd7-0cfd7a385b5b\n\nSave the predictions to disk\n\n    predictions.to_csv(\"../predictions/predictions_\" + str(dt.datetime.strftime(dt.datetime.now(), \"%d%m%Y\")))\n\n22 - c8812eec-1026-4992-b606-be470e1772ca\n\n"]}],"metadata":[["org"],null,null],"nbformat":4,"nbformat_minor":0}
