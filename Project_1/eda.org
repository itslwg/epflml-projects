
  #+BEGIN_SRC elisp :session 
(pyvenv-activate "~/courses/Machine Learning/")
  #+END_SRC

  #+RESULTS:

* EDA
 
  Import packages

#+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
os.chdir("utils")
#+END_SRC

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import os
%load_ext autoreload
%autoreload 2

from utils.implementations import reg_logistic_regression
  #+END_SRC

  Read the training data

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
train = pd.read_csv("../data/train.csv")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[33]:
  :END:

  Replace the missing values by np.nan, and try list-wise deletion

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
cc = train.replace(to_replace=-999, value=np.nan).dropna()
cc.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[35]:
  : (68114, 32)
  :END:

  We are ultimately left with a sample of 68 thousand rows, which should be an adequate sample size for prediction.
  We also want to investigate the balance of the outcome. First, subset and coerce the label vector to numeric
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
y = np.where(np.asarray(cc.loc[:, 'Prediction']) == 's', 1, 0)
y
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[21]:
  : array([1, 0, 1, ..., 1, 1, 0])
  :END:

  Plot the outcome variable

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
unique, counts = np.unique(y, return_counts=True)
plt.bar(unique, counts)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[36]:
  : <BarContainer object of 2 artists>
  [[file:./obipy-resources/sM9IjD.png]]
  :END:

  We have approximately 30 thousand labels that are classified as 1, and we see that the outcome is balanced among 1s 
  ans 0s. Hence, we are able to evaluate the model on a balanced outcome.
  
  Now we create the feature set. Drop the Prediction and the id columns to create the feature matrix
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
c = ['Id', 'Prediction']
X = cc.drop(columns=c)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[37]:
  :END:
  
  Standardize the feature set for prediction

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[38]:
  :END:

  Augment the dataset with 1s, for the intercept of the model.

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X_model = pd.concat([
    pd.DataFrame(
        np.ones((X_standardized.shape[0], 1)), 
        columns=['beta0'], 
        index=X_standardized.index
    ),
    X_standardized
], axis=1)
X_model.iloc[:5, :5]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[40]:
  #+BEGIN_EXAMPLE
    beta0  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h
    0     1.0      0.257669                     0.532831      0.474607 -1.093446
    5     1.0     -0.544384                    -0.706345     -0.475040  0.064383
    6     1.0      0.426949                    -0.208398      0.719029 -0.069451
    11    1.0     -0.132872                    -0.812491     -0.068375 -1.056286
    23    1.0      0.307232                    -1.123057      0.812305  0.820829
  #+END_EXAMPLE
  :END:
  
  To conduct the same data-preprocessing on the test set, we prepare a function for these operations
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def create_prediction_data(raw_sample, include_outcome=True):
    """Missing data handling, data subsettting, and augmentation."""
    ## Drop nas
    cc = raw_sample.replace(to_replace=-999, value=np.nan).dropna()
    ## Subset outcome 
    y = None
    if include_outcome:
        y = np.where(np.asarray(cc.loc[:, 'Prediction']) == 's', 1, 0)
    c = ['Id', 'Prediction']
    X = cc.drop(columns=c)
    ## Standardize sample
    X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)
    ## Make prediction data
    X_model = pd.concat([
        pd.DataFrame(
            np.ones((X_standardized.shape[0], 1)), 
            columns=['beta0'], 
            index=X_standardized.index
        ),
        X_standardized
    ], axis=1)

    return X_model, y
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[42]:
  :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X, y = create_prediction_data(train)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[45]:
  :END:

** Exploring the distributions of the variables

   #+BEGIN_SRC ipython :session :exports both :results drawer :async t
f = X.hist(figsize=(15, 15), bins = 100)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[46]:
   [[file:./obipy-resources/XRq7rc.png]]
   :END:
   
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
corr = X.corr()
f, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(
    corr, 
    xticklabels=corr.columns.values,
    yticklabels=corr.columns.values,
    ax=ax
)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   : <AxesSubplot:>
   [[file:./obipy-resources/k3dhF2.png]]
   :END:
   
* Training

  #+RESULTS:
  :RESULTS:
  # Out[200]:
  :END:
  
#+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
X, y = create_prediction_data(train)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[201]:
:END:

#+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
w, loss = logistic_regression(
    y=y.reshape(-1, 1),
    tx=np.asarray(X),
    initial_w=np.array([0 for x in X.columns])[np.newaxis].T,
    max_iters=100,
    gamma=0.000001,
    batch_size = 1
)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[212]:
: 0.5152807244229209
:END:
